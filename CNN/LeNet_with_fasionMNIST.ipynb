{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"../fasionMNIST/data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../fasionMNIST/data\",\n",
    "    train=False,\n",
    "    download=False, \n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # 28 - 5 + 1 = 24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 24 / 2 = 12\n",
    "            nn.Conv2d(6, 16, 5), # 12 - 5 + 1 = 8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 8 / 2 = 4\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),  # 十六通道特征图，每个都是4 × 4大小\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),  \n",
    "        )\n",
    "    def forward(self, X):\n",
    "        feature = self.conv(X)\n",
    "        output = self.fc(feature.view(X.shape[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([6, 1, 5, 5])\n",
      "0.bias torch.Size([6])\n",
      "3.weight torch.Size([16, 6, 5, 5])\n",
      "3.bias torch.Size([16])\n",
      "\n",
      "0.weight torch.Size([120, 256])\n",
      "0.bias torch.Size([120])\n",
      "2.weight torch.Size([84, 120])\n",
      "2.bias torch.Size([84])\n",
      "4.weight torch.Size([10, 84])\n",
      "4.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.conv.named_parameters():\n",
    "    print(name, param.shape)\n",
    "print()\n",
    "for name, param in model.fc.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:  \n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:.6f}  [{current:5d}/{size:5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).float().sum().item()\n",
    "        loss /= num_batches\n",
    "        loss_list.append(loss)\n",
    "        correct /= size\n",
    "    print(\"Avg Loss: {:.6f}\\tAccuracy: {:.2f}%\".format(loss, correct*100))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------\n",
      "loss: 2.295106  [    0/60000]\n",
      "loss: 2.299838  [ 6400/60000]\n",
      "loss: 2.299732  [12800/60000]\n",
      "loss: 2.297397  [19200/60000]\n",
      "loss: 2.298681  [25600/60000]\n",
      "loss: 2.285313  [32000/60000]\n",
      "loss: 2.273310  [38400/60000]\n",
      "loss: 2.222434  [44800/60000]\n",
      "loss: 1.989527  [51200/60000]\n",
      "loss: 1.274364  [57600/60000]\n",
      "Avg Loss: 1.153266\tAccuracy: 58.89%\n",
      "Epoch 2\n",
      "-------------\n",
      "loss: 1.236359  [    0/60000]\n",
      "loss: 1.059353  [ 6400/60000]\n",
      "loss: 0.926369  [12800/60000]\n",
      "loss: 1.003245  [19200/60000]\n",
      "loss: 0.916539  [25600/60000]\n",
      "loss: 0.870910  [32000/60000]\n",
      "loss: 0.851116  [38400/60000]\n",
      "loss: 0.815461  [44800/60000]\n",
      "loss: 0.776769  [51200/60000]\n",
      "loss: 0.861566  [57600/60000]\n",
      "Avg Loss: 0.841251\tAccuracy: 68.32%\n",
      "Epoch 3\n",
      "-------------\n",
      "loss: 0.866555  [    0/60000]\n",
      "loss: 0.824368  [ 6400/60000]\n",
      "loss: 0.570526  [12800/60000]\n",
      "loss: 0.810809  [19200/60000]\n",
      "loss: 0.726017  [25600/60000]\n",
      "loss: 0.780014  [32000/60000]\n",
      "loss: 0.709061  [38400/60000]\n",
      "loss: 0.661647  [44800/60000]\n",
      "loss: 0.674285  [51200/60000]\n",
      "loss: 0.751515  [57600/60000]\n",
      "Avg Loss: 0.729209\tAccuracy: 73.27%\n",
      "Epoch 4\n",
      "-------------\n",
      "loss: 0.742434  [    0/60000]\n",
      "loss: 0.716635  [ 6400/60000]\n",
      "loss: 0.478897  [12800/60000]\n",
      "loss: 0.728534  [19200/60000]\n",
      "loss: 0.652556  [25600/60000]\n",
      "loss: 0.717906  [32000/60000]\n",
      "loss: 0.635385  [38400/60000]\n",
      "loss: 0.606857  [44800/60000]\n",
      "loss: 0.609528  [51200/60000]\n",
      "loss: 0.670802  [57600/60000]\n",
      "Avg Loss: 0.660939\tAccuracy: 75.79%\n",
      "Epoch 5\n",
      "-------------\n",
      "loss: 0.654458  [    0/60000]\n",
      "loss: 0.657904  [ 6400/60000]\n",
      "loss: 0.429929  [12800/60000]\n",
      "loss: 0.662057  [19200/60000]\n",
      "loss: 0.627578  [25600/60000]\n",
      "loss: 0.643367  [32000/60000]\n",
      "loss: 0.600199  [38400/60000]\n",
      "loss: 0.586166  [44800/60000]\n",
      "loss: 0.581791  [51200/60000]\n",
      "loss: 0.612786  [57600/60000]\n",
      "Avg Loss: 0.617829\tAccuracy: 77.31%\n",
      "Epoch 6\n",
      "-------------\n",
      "loss: 0.583084  [    0/60000]\n",
      "loss: 0.623270  [ 6400/60000]\n",
      "loss: 0.403190  [12800/60000]\n",
      "loss: 0.603502  [19200/60000]\n",
      "loss: 0.628702  [25600/60000]\n",
      "loss: 0.589259  [32000/60000]\n",
      "loss: 0.572688  [38400/60000]\n",
      "loss: 0.584689  [44800/60000]\n",
      "loss: 0.566407  [51200/60000]\n",
      "loss: 0.571488  [57600/60000]\n",
      "Avg Loss: 0.585928\tAccuracy: 78.66%\n",
      "Epoch 7\n",
      "-------------\n",
      "loss: 0.537818  [    0/60000]\n",
      "loss: 0.594106  [ 6400/60000]\n",
      "loss: 0.383539  [12800/60000]\n",
      "loss: 0.566227  [19200/60000]\n",
      "loss: 0.617759  [25600/60000]\n",
      "loss: 0.541791  [32000/60000]\n",
      "loss: 0.542025  [38400/60000]\n",
      "loss: 0.572374  [44800/60000]\n",
      "loss: 0.554459  [51200/60000]\n",
      "loss: 0.543473  [57600/60000]\n",
      "Avg Loss: 0.557483\tAccuracy: 79.97%\n",
      "Epoch 8\n",
      "-------------\n",
      "loss: 0.504902  [    0/60000]\n",
      "loss: 0.556915  [ 6400/60000]\n",
      "loss: 0.359245  [12800/60000]\n",
      "loss: 0.539471  [19200/60000]\n",
      "loss: 0.598333  [25600/60000]\n",
      "loss: 0.508293  [32000/60000]\n",
      "loss: 0.509286  [38400/60000]\n",
      "loss: 0.568405  [44800/60000]\n",
      "loss: 0.552227  [51200/60000]\n",
      "loss: 0.521050  [57600/60000]\n",
      "Avg Loss: 0.530217\tAccuracy: 81.20%\n",
      "Epoch 9\n",
      "-------------\n",
      "loss: 0.471093  [    0/60000]\n",
      "loss: 0.531364  [ 6400/60000]\n",
      "loss: 0.338534  [12800/60000]\n",
      "loss: 0.513658  [19200/60000]\n",
      "loss: 0.585007  [25600/60000]\n",
      "loss: 0.482696  [32000/60000]\n",
      "loss: 0.483405  [38400/60000]\n",
      "loss: 0.563432  [44800/60000]\n",
      "loss: 0.552095  [51200/60000]\n",
      "loss: 0.504461  [57600/60000]\n",
      "Avg Loss: 0.507466\tAccuracy: 82.04%\n",
      "Epoch 10\n",
      "-------------\n",
      "loss: 0.450920  [    0/60000]\n",
      "loss: 0.501012  [ 6400/60000]\n",
      "loss: 0.324883  [12800/60000]\n",
      "loss: 0.494669  [19200/60000]\n",
      "loss: 0.569158  [25600/60000]\n",
      "loss: 0.468604  [32000/60000]\n",
      "loss: 0.468186  [38400/60000]\n",
      "loss: 0.554846  [44800/60000]\n",
      "loss: 0.550870  [51200/60000]\n",
      "loss: 0.481715  [57600/60000]\n",
      "Avg Loss: 0.488218\tAccuracy: 82.59%\n",
      "Epoch 11\n",
      "-------------\n",
      "loss: 0.430375  [    0/60000]\n",
      "loss: 0.473929  [ 6400/60000]\n",
      "loss: 0.307873  [12800/60000]\n",
      "loss: 0.477259  [19200/60000]\n",
      "loss: 0.548050  [25600/60000]\n",
      "loss: 0.453714  [32000/60000]\n",
      "loss: 0.451788  [38400/60000]\n",
      "loss: 0.537181  [44800/60000]\n",
      "loss: 0.541411  [51200/60000]\n",
      "loss: 0.467076  [57600/60000]\n",
      "Avg Loss: 0.472474\tAccuracy: 82.83%\n",
      "Epoch 12\n",
      "-------------\n",
      "loss: 0.418046  [    0/60000]\n",
      "loss: 0.445667  [ 6400/60000]\n",
      "loss: 0.291114  [12800/60000]\n",
      "loss: 0.462714  [19200/60000]\n",
      "loss: 0.537674  [25600/60000]\n",
      "loss: 0.447011  [32000/60000]\n",
      "loss: 0.437853  [38400/60000]\n",
      "loss: 0.530091  [44800/60000]\n",
      "loss: 0.542604  [51200/60000]\n",
      "loss: 0.449881  [57600/60000]\n",
      "Avg Loss: 0.455080\tAccuracy: 83.58%\n",
      "Epoch 13\n",
      "-------------\n",
      "loss: 0.398122  [    0/60000]\n",
      "loss: 0.417088  [ 6400/60000]\n",
      "loss: 0.286073  [12800/60000]\n",
      "loss: 0.443562  [19200/60000]\n",
      "loss: 0.526441  [25600/60000]\n",
      "loss: 0.444453  [32000/60000]\n",
      "loss: 0.421739  [38400/60000]\n",
      "loss: 0.526525  [44800/60000]\n",
      "loss: 0.543408  [51200/60000]\n",
      "loss: 0.442842  [57600/60000]\n",
      "Avg Loss: 0.441043\tAccuracy: 83.99%\n",
      "Epoch 14\n",
      "-------------\n",
      "loss: 0.383758  [    0/60000]\n",
      "loss: 0.397897  [ 6400/60000]\n",
      "loss: 0.280865  [12800/60000]\n",
      "loss: 0.423117  [19200/60000]\n",
      "loss: 0.503307  [25600/60000]\n",
      "loss: 0.440898  [32000/60000]\n",
      "loss: 0.418815  [38400/60000]\n",
      "loss: 0.515461  [44800/60000]\n",
      "loss: 0.528207  [51200/60000]\n",
      "loss: 0.432421  [57600/60000]\n",
      "Avg Loss: 0.432381\tAccuracy: 84.29%\n",
      "Epoch 15\n",
      "-------------\n",
      "loss: 0.377750  [    0/60000]\n",
      "loss: 0.384550  [ 6400/60000]\n",
      "loss: 0.276558  [12800/60000]\n",
      "loss: 0.411976  [19200/60000]\n",
      "loss: 0.497342  [25600/60000]\n",
      "loss: 0.422360  [32000/60000]\n",
      "loss: 0.409351  [38400/60000]\n",
      "loss: 0.513550  [44800/60000]\n",
      "loss: 0.527454  [51200/60000]\n",
      "loss: 0.425530  [57600/60000]\n",
      "Avg Loss: 0.426368\tAccuracy: 84.37%\n",
      "Epoch 16\n",
      "-------------\n",
      "loss: 0.377458  [    0/60000]\n",
      "loss: 0.378458  [ 6400/60000]\n",
      "loss: 0.271797  [12800/60000]\n",
      "loss: 0.395408  [19200/60000]\n",
      "loss: 0.487059  [25600/60000]\n",
      "loss: 0.422915  [32000/60000]\n",
      "loss: 0.397156  [38400/60000]\n",
      "loss: 0.507442  [44800/60000]\n",
      "loss: 0.509531  [51200/60000]\n",
      "loss: 0.424956  [57600/60000]\n",
      "Avg Loss: 0.413409\tAccuracy: 84.76%\n",
      "Epoch 17\n",
      "-------------\n",
      "loss: 0.365727  [    0/60000]\n",
      "loss: 0.366704  [ 6400/60000]\n",
      "loss: 0.269513  [12800/60000]\n",
      "loss: 0.384440  [19200/60000]\n",
      "loss: 0.474711  [25600/60000]\n",
      "loss: 0.409622  [32000/60000]\n",
      "loss: 0.386482  [38400/60000]\n",
      "loss: 0.499016  [44800/60000]\n",
      "loss: 0.506036  [51200/60000]\n",
      "loss: 0.416689  [57600/60000]\n",
      "Avg Loss: 0.405472\tAccuracy: 85.17%\n",
      "Epoch 18\n",
      "-------------\n",
      "loss: 0.358312  [    0/60000]\n",
      "loss: 0.357227  [ 6400/60000]\n",
      "loss: 0.265631  [12800/60000]\n",
      "loss: 0.381113  [19200/60000]\n",
      "loss: 0.458574  [25600/60000]\n",
      "loss: 0.400720  [32000/60000]\n",
      "loss: 0.377143  [38400/60000]\n",
      "loss: 0.486794  [44800/60000]\n",
      "loss: 0.499740  [51200/60000]\n",
      "loss: 0.401039  [57600/60000]\n",
      "Avg Loss: 0.398459\tAccuracy: 85.49%\n",
      "Epoch 19\n",
      "-------------\n",
      "loss: 0.348390  [    0/60000]\n",
      "loss: 0.350388  [ 6400/60000]\n",
      "loss: 0.264109  [12800/60000]\n",
      "loss: 0.368758  [19200/60000]\n",
      "loss: 0.448521  [25600/60000]\n",
      "loss: 0.387349  [32000/60000]\n",
      "loss: 0.364970  [38400/60000]\n",
      "loss: 0.484559  [44800/60000]\n",
      "loss: 0.496367  [51200/60000]\n",
      "loss: 0.402174  [57600/60000]\n",
      "Avg Loss: 0.393596\tAccuracy: 85.51%\n",
      "Epoch 20\n",
      "-------------\n",
      "loss: 0.342389  [    0/60000]\n",
      "loss: 0.336731  [ 6400/60000]\n",
      "loss: 0.264851  [12800/60000]\n",
      "loss: 0.364628  [19200/60000]\n",
      "loss: 0.421676  [25600/60000]\n",
      "loss: 0.384202  [32000/60000]\n",
      "loss: 0.358133  [38400/60000]\n",
      "loss: 0.484828  [44800/60000]\n",
      "loss: 0.487271  [51200/60000]\n",
      "loss: 0.396807  [57600/60000]\n",
      "Avg Loss: 0.388508\tAccuracy: 85.71%\n",
      "Epoch 21\n",
      "-------------\n",
      "loss: 0.336248  [    0/60000]\n",
      "loss: 0.328707  [ 6400/60000]\n",
      "loss: 0.258074  [12800/60000]\n",
      "loss: 0.354999  [19200/60000]\n",
      "loss: 0.414063  [25600/60000]\n",
      "loss: 0.375574  [32000/60000]\n",
      "loss: 0.348174  [38400/60000]\n",
      "loss: 0.482141  [44800/60000]\n",
      "loss: 0.483123  [51200/60000]\n",
      "loss: 0.397822  [57600/60000]\n",
      "Avg Loss: 0.385783\tAccuracy: 85.65%\n",
      "Epoch 22\n",
      "-------------\n",
      "loss: 0.331424  [    0/60000]\n",
      "loss: 0.325589  [ 6400/60000]\n",
      "loss: 0.255819  [12800/60000]\n",
      "loss: 0.347401  [19200/60000]\n",
      "loss: 0.401704  [25600/60000]\n",
      "loss: 0.369300  [32000/60000]\n",
      "loss: 0.344169  [38400/60000]\n",
      "loss: 0.479551  [44800/60000]\n",
      "loss: 0.487389  [51200/60000]\n",
      "loss: 0.393792  [57600/60000]\n",
      "Avg Loss: 0.375123\tAccuracy: 86.22%\n",
      "Epoch 23\n",
      "-------------\n",
      "loss: 0.319653  [    0/60000]\n",
      "loss: 0.324035  [ 6400/60000]\n",
      "loss: 0.251991  [12800/60000]\n",
      "loss: 0.340886  [19200/60000]\n",
      "loss: 0.388208  [25600/60000]\n",
      "loss: 0.369152  [32000/60000]\n",
      "loss: 0.341837  [38400/60000]\n",
      "loss: 0.473845  [44800/60000]\n",
      "loss: 0.485365  [51200/60000]\n",
      "loss: 0.399888  [57600/60000]\n",
      "Avg Loss: 0.372682\tAccuracy: 86.22%\n",
      "Epoch 24\n",
      "-------------\n",
      "loss: 0.315462  [    0/60000]\n",
      "loss: 0.316546  [ 6400/60000]\n",
      "loss: 0.236944  [12800/60000]\n",
      "loss: 0.339484  [19200/60000]\n",
      "loss: 0.374694  [25600/60000]\n",
      "loss: 0.367989  [32000/60000]\n",
      "loss: 0.343197  [38400/60000]\n",
      "loss: 0.483104  [44800/60000]\n",
      "loss: 0.471126  [51200/60000]\n",
      "loss: 0.404199  [57600/60000]\n",
      "Avg Loss: 0.371649\tAccuracy: 86.18%\n",
      "Epoch 25\n",
      "-------------\n",
      "loss: 0.311450  [    0/60000]\n",
      "loss: 0.310573  [ 6400/60000]\n",
      "loss: 0.239451  [12800/60000]\n",
      "loss: 0.332212  [19200/60000]\n",
      "loss: 0.361857  [25600/60000]\n",
      "loss: 0.364201  [32000/60000]\n",
      "loss: 0.333099  [38400/60000]\n",
      "loss: 0.475304  [44800/60000]\n",
      "loss: 0.467988  [51200/60000]\n",
      "loss: 0.403895  [57600/60000]\n",
      "Avg Loss: 0.363904\tAccuracy: 86.56%\n",
      "Epoch 26\n",
      "-------------\n",
      "loss: 0.295661  [    0/60000]\n",
      "loss: 0.310184  [ 6400/60000]\n",
      "loss: 0.236254  [12800/60000]\n",
      "loss: 0.328996  [19200/60000]\n",
      "loss: 0.358454  [25600/60000]\n",
      "loss: 0.363895  [32000/60000]\n",
      "loss: 0.322783  [38400/60000]\n",
      "loss: 0.476811  [44800/60000]\n",
      "loss: 0.465646  [51200/60000]\n",
      "loss: 0.401935  [57600/60000]\n",
      "Avg Loss: 0.362808\tAccuracy: 86.50%\n",
      "Epoch 27\n",
      "-------------\n",
      "loss: 0.295475  [    0/60000]\n",
      "loss: 0.305036  [ 6400/60000]\n",
      "loss: 0.225564  [12800/60000]\n",
      "loss: 0.324974  [19200/60000]\n",
      "loss: 0.344723  [25600/60000]\n",
      "loss: 0.358300  [32000/60000]\n",
      "loss: 0.316184  [38400/60000]\n",
      "loss: 0.471163  [44800/60000]\n",
      "loss: 0.454660  [51200/60000]\n",
      "loss: 0.393777  [57600/60000]\n",
      "Avg Loss: 0.355355\tAccuracy: 86.94%\n",
      "Epoch 28\n",
      "-------------\n",
      "loss: 0.284446  [    0/60000]\n",
      "loss: 0.299571  [ 6400/60000]\n",
      "loss: 0.224858  [12800/60000]\n",
      "loss: 0.318148  [19200/60000]\n",
      "loss: 0.334861  [25600/60000]\n",
      "loss: 0.357334  [32000/60000]\n",
      "loss: 0.309501  [38400/60000]\n",
      "loss: 0.467742  [44800/60000]\n",
      "loss: 0.460454  [51200/60000]\n",
      "loss: 0.396359  [57600/60000]\n",
      "Avg Loss: 0.350660\tAccuracy: 87.10%\n",
      "Epoch 29\n",
      "-------------\n",
      "loss: 0.275479  [    0/60000]\n",
      "loss: 0.300329  [ 6400/60000]\n",
      "loss: 0.217245  [12800/60000]\n",
      "loss: 0.313181  [19200/60000]\n",
      "loss: 0.328633  [25600/60000]\n",
      "loss: 0.360302  [32000/60000]\n",
      "loss: 0.308622  [38400/60000]\n",
      "loss: 0.456661  [44800/60000]\n",
      "loss: 0.452418  [51200/60000]\n",
      "loss: 0.384416  [57600/60000]\n",
      "Avg Loss: 0.346981\tAccuracy: 87.21%\n",
      "Epoch 30\n",
      "-------------\n",
      "loss: 0.268504  [    0/60000]\n",
      "loss: 0.295887  [ 6400/60000]\n",
      "loss: 0.209336  [12800/60000]\n",
      "loss: 0.310559  [19200/60000]\n",
      "loss: 0.318697  [25600/60000]\n",
      "loss: 0.354485  [32000/60000]\n",
      "loss: 0.302138  [38400/60000]\n",
      "loss: 0.457468  [44800/60000]\n",
      "loss: 0.456345  [51200/60000]\n",
      "loss: 0.384512  [57600/60000]\n",
      "Avg Loss: 0.343681\tAccuracy: 87.49%\n",
      "Epoch 31\n",
      "-------------\n",
      "loss: 0.266778  [    0/60000]\n",
      "loss: 0.297355  [ 6400/60000]\n",
      "loss: 0.208222  [12800/60000]\n",
      "loss: 0.308617  [19200/60000]\n",
      "loss: 0.309068  [25600/60000]\n",
      "loss: 0.352725  [32000/60000]\n",
      "loss: 0.293691  [38400/60000]\n",
      "loss: 0.442901  [44800/60000]\n",
      "loss: 0.458000  [51200/60000]\n",
      "loss: 0.380186  [57600/60000]\n",
      "Avg Loss: 0.343392\tAccuracy: 87.37%\n",
      "Epoch 32\n",
      "-------------\n",
      "loss: 0.265377  [    0/60000]\n",
      "loss: 0.294892  [ 6400/60000]\n",
      "loss: 0.203088  [12800/60000]\n",
      "loss: 0.301284  [19200/60000]\n",
      "loss: 0.305761  [25600/60000]\n",
      "loss: 0.349438  [32000/60000]\n",
      "loss: 0.294435  [38400/60000]\n",
      "loss: 0.434790  [44800/60000]\n",
      "loss: 0.459085  [51200/60000]\n",
      "loss: 0.382898  [57600/60000]\n",
      "Avg Loss: 0.341268\tAccuracy: 87.34%\n",
      "Epoch 33\n",
      "-------------\n",
      "loss: 0.267167  [    0/60000]\n",
      "loss: 0.295776  [ 6400/60000]\n",
      "loss: 0.197579  [12800/60000]\n",
      "loss: 0.298888  [19200/60000]\n",
      "loss: 0.295054  [25600/60000]\n",
      "loss: 0.344748  [32000/60000]\n",
      "loss: 0.285165  [38400/60000]\n",
      "loss: 0.430821  [44800/60000]\n",
      "loss: 0.450927  [51200/60000]\n",
      "loss: 0.373854  [57600/60000]\n",
      "Avg Loss: 0.336949\tAccuracy: 87.65%\n",
      "Epoch 34\n",
      "-------------\n",
      "loss: 0.254549  [    0/60000]\n",
      "loss: 0.294982  [ 6400/60000]\n",
      "loss: 0.207239  [12800/60000]\n",
      "loss: 0.295768  [19200/60000]\n",
      "loss: 0.288524  [25600/60000]\n",
      "loss: 0.344082  [32000/60000]\n",
      "loss: 0.291499  [38400/60000]\n",
      "loss: 0.417556  [44800/60000]\n",
      "loss: 0.458435  [51200/60000]\n",
      "loss: 0.372745  [57600/60000]\n",
      "Avg Loss: 0.335698\tAccuracy: 87.82%\n",
      "Epoch 35\n",
      "-------------\n",
      "loss: 0.254987  [    0/60000]\n",
      "loss: 0.296116  [ 6400/60000]\n",
      "loss: 0.203508  [12800/60000]\n",
      "loss: 0.290019  [19200/60000]\n",
      "loss: 0.287907  [25600/60000]\n",
      "loss: 0.343974  [32000/60000]\n",
      "loss: 0.287689  [38400/60000]\n",
      "loss: 0.403977  [44800/60000]\n",
      "loss: 0.452426  [51200/60000]\n",
      "loss: 0.374427  [57600/60000]\n",
      "Avg Loss: 0.335075\tAccuracy: 87.87%\n",
      "Epoch 36\n",
      "-------------\n",
      "loss: 0.253156  [    0/60000]\n",
      "loss: 0.296962  [ 6400/60000]\n",
      "loss: 0.202049  [12800/60000]\n",
      "loss: 0.283919  [19200/60000]\n",
      "loss: 0.291477  [25600/60000]\n",
      "loss: 0.334698  [32000/60000]\n",
      "loss: 0.285568  [38400/60000]\n",
      "loss: 0.402788  [44800/60000]\n",
      "loss: 0.456603  [51200/60000]\n",
      "loss: 0.370578  [57600/60000]\n",
      "Avg Loss: 0.331889\tAccuracy: 87.98%\n",
      "Epoch 37\n",
      "-------------\n",
      "loss: 0.245538  [    0/60000]\n",
      "loss: 0.288454  [ 6400/60000]\n",
      "loss: 0.197137  [12800/60000]\n",
      "loss: 0.279833  [19200/60000]\n",
      "loss: 0.284817  [25600/60000]\n",
      "loss: 0.336364  [32000/60000]\n",
      "loss: 0.285794  [38400/60000]\n",
      "loss: 0.385910  [44800/60000]\n",
      "loss: 0.433381  [51200/60000]\n",
      "loss: 0.370703  [57600/60000]\n",
      "Avg Loss: 0.330959\tAccuracy: 88.09%\n",
      "Epoch 38\n",
      "-------------\n",
      "loss: 0.245416  [    0/60000]\n",
      "loss: 0.288577  [ 6400/60000]\n",
      "loss: 0.190615  [12800/60000]\n",
      "loss: 0.274538  [19200/60000]\n",
      "loss: 0.285645  [25600/60000]\n",
      "loss: 0.334612  [32000/60000]\n",
      "loss: 0.284585  [38400/60000]\n",
      "loss: 0.381753  [44800/60000]\n",
      "loss: 0.435744  [51200/60000]\n",
      "loss: 0.374241  [57600/60000]\n",
      "Avg Loss: 0.327555\tAccuracy: 88.23%\n",
      "Epoch 39\n",
      "-------------\n",
      "loss: 0.240561  [    0/60000]\n",
      "loss: 0.287546  [ 6400/60000]\n",
      "loss: 0.188364  [12800/60000]\n",
      "loss: 0.271112  [19200/60000]\n",
      "loss: 0.276158  [25600/60000]\n",
      "loss: 0.334245  [32000/60000]\n",
      "loss: 0.282370  [38400/60000]\n",
      "loss: 0.373054  [44800/60000]\n",
      "loss: 0.420532  [51200/60000]\n",
      "loss: 0.366940  [57600/60000]\n",
      "Avg Loss: 0.325995\tAccuracy: 88.31%\n",
      "Epoch 40\n",
      "-------------\n",
      "loss: 0.242965  [    0/60000]\n",
      "loss: 0.290354  [ 6400/60000]\n",
      "loss: 0.198562  [12800/60000]\n",
      "loss: 0.263578  [19200/60000]\n",
      "loss: 0.284551  [25600/60000]\n",
      "loss: 0.327995  [32000/60000]\n",
      "loss: 0.286153  [38400/60000]\n",
      "loss: 0.370551  [44800/60000]\n",
      "loss: 0.418523  [51200/60000]\n",
      "loss: 0.361488  [57600/60000]\n",
      "Avg Loss: 0.324471\tAccuracy: 88.47%\n",
      "Epoch 41\n",
      "-------------\n",
      "loss: 0.244275  [    0/60000]\n",
      "loss: 0.291235  [ 6400/60000]\n",
      "loss: 0.191915  [12800/60000]\n",
      "loss: 0.260609  [19200/60000]\n",
      "loss: 0.276821  [25600/60000]\n",
      "loss: 0.330839  [32000/60000]\n",
      "loss: 0.284980  [38400/60000]\n",
      "loss: 0.362165  [44800/60000]\n",
      "loss: 0.421468  [51200/60000]\n",
      "loss: 0.355004  [57600/60000]\n",
      "Avg Loss: 0.322240\tAccuracy: 88.53%\n",
      "Epoch 42\n",
      "-------------\n",
      "loss: 0.239239  [    0/60000]\n",
      "loss: 0.295516  [ 6400/60000]\n",
      "loss: 0.184893  [12800/60000]\n",
      "loss: 0.252233  [19200/60000]\n",
      "loss: 0.265723  [25600/60000]\n",
      "loss: 0.326109  [32000/60000]\n",
      "loss: 0.280782  [38400/60000]\n",
      "loss: 0.358928  [44800/60000]\n",
      "loss: 0.411728  [51200/60000]\n",
      "loss: 0.363493  [57600/60000]\n",
      "Avg Loss: 0.321185\tAccuracy: 88.56%\n",
      "Epoch 43\n",
      "-------------\n",
      "loss: 0.237458  [    0/60000]\n",
      "loss: 0.294554  [ 6400/60000]\n",
      "loss: 0.188491  [12800/60000]\n",
      "loss: 0.249609  [19200/60000]\n",
      "loss: 0.274721  [25600/60000]\n",
      "loss: 0.322467  [32000/60000]\n",
      "loss: 0.278167  [38400/60000]\n",
      "loss: 0.351385  [44800/60000]\n",
      "loss: 0.401196  [51200/60000]\n",
      "loss: 0.356815  [57600/60000]\n",
      "Avg Loss: 0.320305\tAccuracy: 88.58%\n",
      "Epoch 44\n",
      "-------------\n",
      "loss: 0.234164  [    0/60000]\n",
      "loss: 0.290749  [ 6400/60000]\n",
      "loss: 0.186218  [12800/60000]\n",
      "loss: 0.248089  [19200/60000]\n",
      "loss: 0.271474  [25600/60000]\n",
      "loss: 0.323247  [32000/60000]\n",
      "loss: 0.272775  [38400/60000]\n",
      "loss: 0.339919  [44800/60000]\n",
      "loss: 0.403140  [51200/60000]\n",
      "loss: 0.353713  [57600/60000]\n",
      "Avg Loss: 0.318866\tAccuracy: 88.74%\n",
      "Epoch 45\n",
      "-------------\n",
      "loss: 0.234726  [    0/60000]\n",
      "loss: 0.294602  [ 6400/60000]\n",
      "loss: 0.195145  [12800/60000]\n",
      "loss: 0.246365  [19200/60000]\n",
      "loss: 0.264655  [25600/60000]\n",
      "loss: 0.317464  [32000/60000]\n",
      "loss: 0.263655  [38400/60000]\n",
      "loss: 0.332688  [44800/60000]\n",
      "loss: 0.399315  [51200/60000]\n",
      "loss: 0.350255  [57600/60000]\n",
      "Avg Loss: 0.317446\tAccuracy: 88.68%\n",
      "Epoch 46\n",
      "-------------\n",
      "loss: 0.239425  [    0/60000]\n",
      "loss: 0.288912  [ 6400/60000]\n",
      "loss: 0.188298  [12800/60000]\n",
      "loss: 0.237229  [19200/60000]\n",
      "loss: 0.262024  [25600/60000]\n",
      "loss: 0.313719  [32000/60000]\n",
      "loss: 0.262850  [38400/60000]\n",
      "loss: 0.327025  [44800/60000]\n",
      "loss: 0.393462  [51200/60000]\n",
      "loss: 0.351100  [57600/60000]\n",
      "Avg Loss: 0.317494\tAccuracy: 88.68%\n",
      "Epoch 47\n",
      "-------------\n",
      "loss: 0.236410  [    0/60000]\n",
      "loss: 0.294950  [ 6400/60000]\n",
      "loss: 0.185056  [12800/60000]\n",
      "loss: 0.239503  [19200/60000]\n",
      "loss: 0.268324  [25600/60000]\n",
      "loss: 0.314822  [32000/60000]\n",
      "loss: 0.258053  [38400/60000]\n",
      "loss: 0.323998  [44800/60000]\n",
      "loss: 0.386277  [51200/60000]\n",
      "loss: 0.351875  [57600/60000]\n",
      "Avg Loss: 0.316690\tAccuracy: 88.68%\n",
      "Epoch 48\n",
      "-------------\n",
      "loss: 0.240521  [    0/60000]\n",
      "loss: 0.293804  [ 6400/60000]\n",
      "loss: 0.178785  [12800/60000]\n",
      "loss: 0.231049  [19200/60000]\n",
      "loss: 0.259067  [25600/60000]\n",
      "loss: 0.314418  [32000/60000]\n",
      "loss: 0.259177  [38400/60000]\n",
      "loss: 0.315939  [44800/60000]\n",
      "loss: 0.385884  [51200/60000]\n",
      "loss: 0.348331  [57600/60000]\n",
      "Avg Loss: 0.315683\tAccuracy: 88.70%\n",
      "Epoch 49\n",
      "-------------\n",
      "loss: 0.235995  [    0/60000]\n",
      "loss: 0.294996  [ 6400/60000]\n",
      "loss: 0.176127  [12800/60000]\n",
      "loss: 0.233593  [19200/60000]\n",
      "loss: 0.264493  [25600/60000]\n",
      "loss: 0.317434  [32000/60000]\n",
      "loss: 0.252248  [38400/60000]\n",
      "loss: 0.318300  [44800/60000]\n",
      "loss: 0.368829  [51200/60000]\n",
      "loss: 0.344752  [57600/60000]\n",
      "Avg Loss: 0.315805\tAccuracy: 88.71%\n",
      "Epoch 50\n",
      "-------------\n",
      "loss: 0.243871  [    0/60000]\n",
      "loss: 0.296683  [ 6400/60000]\n",
      "loss: 0.164705  [12800/60000]\n",
      "loss: 0.229400  [19200/60000]\n",
      "loss: 0.264368  [25600/60000]\n",
      "loss: 0.311566  [32000/60000]\n",
      "loss: 0.254247  [38400/60000]\n",
      "loss: 0.313119  [44800/60000]\n",
      "loss: 0.372095  [51200/60000]\n",
      "loss: 0.331222  [57600/60000]\n",
      "Avg Loss: 0.314458\tAccuracy: 88.79%\n"
     ]
    }
   ],
   "source": [
    "epoches = 50\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch {}\\n-------------\".format(epoch+1))\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotLoss(epoches, loss_list):\n",
    "    plt.scatter(range(epoches), loss_list, c='r', s=1, label='Lost of epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Loss with training')\n",
    "    plt.show()\n",
    "\n",
    "plotLoss(epoches, loss_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f184e1a4ef8b843ce9d08d4bad624bd1785f539852e9584bc504f988ebdfd4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
