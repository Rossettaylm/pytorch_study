{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"../fasionMNIST/data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor())\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../fasionMNIST/data\",\n",
    "    train=False,\n",
    "    download=False, \n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # 28 - 5 + 1 = 24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 24 / 2 = 12\n",
    "            nn.Conv2d(6, 16, 5), # 12 - 5 + 1 = 8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) # 8 / 2 = 4\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*4*4, 120),  # 十六通道特征图，每个都是4 × 4大小\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),  \n",
    "        )\n",
    "    def forward(self, X):\n",
    "        feature = self.conv(X)\n",
    "        output = self.fc(feature.view(X.shape[0], -1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([6, 1, 5, 5])\n",
      "0.bias torch.Size([6])\n",
      "3.weight torch.Size([16, 6, 5, 5])\n",
      "3.bias torch.Size([16])\n",
      "\n",
      "0.weight torch.Size([120, 256])\n",
      "0.bias torch.Size([120])\n",
      "2.weight torch.Size([84, 120])\n",
      "2.bias torch.Size([84])\n",
      "4.weight torch.Size([10, 84])\n",
      "4.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.conv.named_parameters():\n",
    "    print(name, param.shape)\n",
    "print()\n",
    "for name, param in model.fc.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:  \n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:.6f}  [{current:5d}/{size:5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).float().sum().item()\n",
    "        loss /= num_batches\n",
    "        loss_list.append(loss)\n",
    "        correct /= size\n",
    "    print(\"Avg Loss: {:.6f}\\tAccuracy: {:.2f}%\".format(loss, correct*100))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------\n",
      "loss: 2.313136  [    0/60000]\n",
      "loss: 2.311588  [ 6400/60000]\n",
      "loss: 2.303491  [12800/60000]\n",
      "loss: 2.296183  [19200/60000]\n",
      "loss: 2.287875  [25600/60000]\n",
      "loss: 2.265647  [32000/60000]\n",
      "loss: 2.235445  [38400/60000]\n",
      "loss: 2.011377  [44800/60000]\n",
      "loss: 1.409049  [51200/60000]\n",
      "loss: 1.096218  [57600/60000]\n",
      "Avg Loss: 1.040571\tAccuracy: 62.05%\n",
      "Epoch 2\n",
      "-------------\n",
      "loss: 1.057278  [    0/60000]\n",
      "loss: 1.021185  [ 6400/60000]\n",
      "loss: 0.789306  [12800/60000]\n",
      "loss: 0.962536  [19200/60000]\n",
      "loss: 0.828527  [25600/60000]\n",
      "loss: 0.816874  [32000/60000]\n",
      "loss: 0.824092  [38400/60000]\n",
      "loss: 0.756816  [44800/60000]\n",
      "loss: 0.728537  [51200/60000]\n",
      "loss: 0.836498  [57600/60000]\n",
      "Avg Loss: 0.787322\tAccuracy: 70.42%\n",
      "Epoch 3\n",
      "-------------\n",
      "loss: 0.767325  [    0/60000]\n",
      "loss: 0.730959  [ 6400/60000]\n",
      "loss: 0.530881  [12800/60000]\n",
      "loss: 0.805673  [19200/60000]\n",
      "loss: 0.665809  [25600/60000]\n",
      "loss: 0.736243  [32000/60000]\n",
      "loss: 0.722770  [38400/60000]\n",
      "loss: 0.626098  [44800/60000]\n",
      "loss: 0.655142  [51200/60000]\n",
      "loss: 0.727991  [57600/60000]\n",
      "Avg Loss: 0.676092\tAccuracy: 74.98%\n",
      "Epoch 4\n",
      "-------------\n",
      "loss: 0.636631  [    0/60000]\n",
      "loss: 0.599376  [ 6400/60000]\n",
      "loss: 0.452136  [12800/60000]\n",
      "loss: 0.754329  [19200/60000]\n",
      "loss: 0.601688  [25600/60000]\n",
      "loss: 0.689253  [32000/60000]\n",
      "loss: 0.674944  [38400/60000]\n",
      "loss: 0.586814  [44800/60000]\n",
      "loss: 0.627801  [51200/60000]\n",
      "loss: 0.641159  [57600/60000]\n",
      "Avg Loss: 0.619129\tAccuracy: 77.79%\n",
      "Epoch 5\n",
      "-------------\n",
      "loss: 0.552829  [    0/60000]\n",
      "loss: 0.542827  [ 6400/60000]\n",
      "loss: 0.407462  [12800/60000]\n",
      "loss: 0.693332  [19200/60000]\n",
      "loss: 0.586792  [25600/60000]\n",
      "loss: 0.638424  [32000/60000]\n",
      "loss: 0.634923  [38400/60000]\n",
      "loss: 0.571008  [44800/60000]\n",
      "loss: 0.611530  [51200/60000]\n",
      "loss: 0.583429  [57600/60000]\n",
      "Avg Loss: 0.581585\tAccuracy: 79.12%\n",
      "Epoch 6\n",
      "-------------\n",
      "loss: 0.503068  [    0/60000]\n",
      "loss: 0.503918  [ 6400/60000]\n",
      "loss: 0.379758  [12800/60000]\n",
      "loss: 0.641714  [19200/60000]\n",
      "loss: 0.585283  [25600/60000]\n",
      "loss: 0.599255  [32000/60000]\n",
      "loss: 0.593875  [38400/60000]\n",
      "loss: 0.551805  [44800/60000]\n",
      "loss: 0.598499  [51200/60000]\n",
      "loss: 0.533214  [57600/60000]\n",
      "Avg Loss: 0.558842\tAccuracy: 80.03%\n",
      "Epoch 7\n",
      "-------------\n",
      "loss: 0.465404  [    0/60000]\n",
      "loss: 0.473829  [ 6400/60000]\n",
      "loss: 0.363959  [12800/60000]\n",
      "loss: 0.600455  [19200/60000]\n",
      "loss: 0.571355  [25600/60000]\n",
      "loss: 0.564617  [32000/60000]\n",
      "loss: 0.551811  [38400/60000]\n",
      "loss: 0.538431  [44800/60000]\n",
      "loss: 0.593043  [51200/60000]\n",
      "loss: 0.496461  [57600/60000]\n",
      "Avg Loss: 0.535475\tAccuracy: 80.95%\n",
      "Epoch 8\n",
      "-------------\n",
      "loss: 0.434357  [    0/60000]\n",
      "loss: 0.456679  [ 6400/60000]\n",
      "loss: 0.344764  [12800/60000]\n",
      "loss: 0.570734  [19200/60000]\n",
      "loss: 0.558594  [25600/60000]\n",
      "loss: 0.529801  [32000/60000]\n",
      "loss: 0.519250  [38400/60000]\n",
      "loss: 0.535459  [44800/60000]\n",
      "loss: 0.589746  [51200/60000]\n",
      "loss: 0.463933  [57600/60000]\n",
      "Avg Loss: 0.515795\tAccuracy: 81.83%\n",
      "Epoch 9\n",
      "-------------\n",
      "loss: 0.413897  [    0/60000]\n",
      "loss: 0.448567  [ 6400/60000]\n",
      "loss: 0.330336  [12800/60000]\n",
      "loss: 0.550365  [19200/60000]\n",
      "loss: 0.540489  [25600/60000]\n",
      "loss: 0.496066  [32000/60000]\n",
      "loss: 0.496183  [38400/60000]\n",
      "loss: 0.544299  [44800/60000]\n",
      "loss: 0.592595  [51200/60000]\n",
      "loss: 0.445581  [57600/60000]\n",
      "Avg Loss: 0.495970\tAccuracy: 82.63%\n",
      "Epoch 10\n",
      "-------------\n",
      "loss: 0.399520  [    0/60000]\n",
      "loss: 0.433478  [ 6400/60000]\n",
      "loss: 0.308132  [12800/60000]\n",
      "loss: 0.539636  [19200/60000]\n",
      "loss: 0.521561  [25600/60000]\n",
      "loss: 0.474814  [32000/60000]\n",
      "loss: 0.466775  [38400/60000]\n",
      "loss: 0.550615  [44800/60000]\n",
      "loss: 0.579083  [51200/60000]\n",
      "loss: 0.429536  [57600/60000]\n",
      "Avg Loss: 0.477711\tAccuracy: 83.33%\n",
      "Epoch 11\n",
      "-------------\n",
      "loss: 0.380521  [    0/60000]\n",
      "loss: 0.423554  [ 6400/60000]\n",
      "loss: 0.282878  [12800/60000]\n",
      "loss: 0.526374  [19200/60000]\n",
      "loss: 0.493210  [25600/60000]\n",
      "loss: 0.458092  [32000/60000]\n",
      "loss: 0.447720  [38400/60000]\n",
      "loss: 0.539355  [44800/60000]\n",
      "loss: 0.568800  [51200/60000]\n",
      "loss: 0.413898  [57600/60000]\n",
      "Avg Loss: 0.463111\tAccuracy: 83.69%\n",
      "Epoch 12\n",
      "-------------\n",
      "loss: 0.365129  [    0/60000]\n",
      "loss: 0.406417  [ 6400/60000]\n",
      "loss: 0.265653  [12800/60000]\n",
      "loss: 0.505570  [19200/60000]\n",
      "loss: 0.468215  [25600/60000]\n",
      "loss: 0.443978  [32000/60000]\n",
      "loss: 0.429526  [38400/60000]\n",
      "loss: 0.543194  [44800/60000]\n",
      "loss: 0.569341  [51200/60000]\n",
      "loss: 0.398827  [57600/60000]\n",
      "Avg Loss: 0.448778\tAccuracy: 84.17%\n",
      "Epoch 13\n",
      "-------------\n",
      "loss: 0.353763  [    0/60000]\n",
      "loss: 0.394636  [ 6400/60000]\n",
      "loss: 0.248627  [12800/60000]\n",
      "loss: 0.488725  [19200/60000]\n",
      "loss: 0.450786  [25600/60000]\n",
      "loss: 0.435052  [32000/60000]\n",
      "loss: 0.414686  [38400/60000]\n",
      "loss: 0.534717  [44800/60000]\n",
      "loss: 0.570064  [51200/60000]\n",
      "loss: 0.386562  [57600/60000]\n",
      "Avg Loss: 0.434453\tAccuracy: 84.59%\n",
      "Epoch 14\n",
      "-------------\n",
      "loss: 0.344875  [    0/60000]\n",
      "loss: 0.382208  [ 6400/60000]\n",
      "loss: 0.235178  [12800/60000]\n",
      "loss: 0.477769  [19200/60000]\n",
      "loss: 0.430155  [25600/60000]\n",
      "loss: 0.432178  [32000/60000]\n",
      "loss: 0.403364  [38400/60000]\n",
      "loss: 0.538922  [44800/60000]\n",
      "loss: 0.557984  [51200/60000]\n",
      "loss: 0.376399  [57600/60000]\n",
      "Avg Loss: 0.426607\tAccuracy: 84.73%\n",
      "Epoch 15\n",
      "-------------\n",
      "loss: 0.337211  [    0/60000]\n",
      "loss: 0.365744  [ 6400/60000]\n",
      "loss: 0.231696  [12800/60000]\n",
      "loss: 0.463490  [19200/60000]\n",
      "loss: 0.424619  [25600/60000]\n",
      "loss: 0.428891  [32000/60000]\n",
      "loss: 0.398390  [38400/60000]\n",
      "loss: 0.533116  [44800/60000]\n",
      "loss: 0.545343  [51200/60000]\n",
      "loss: 0.372701  [57600/60000]\n",
      "Avg Loss: 0.417531\tAccuracy: 84.99%\n",
      "Epoch 16\n",
      "-------------\n",
      "loss: 0.323040  [    0/60000]\n",
      "loss: 0.348551  [ 6400/60000]\n",
      "loss: 0.229633  [12800/60000]\n",
      "loss: 0.452304  [19200/60000]\n",
      "loss: 0.410198  [25600/60000]\n",
      "loss: 0.420039  [32000/60000]\n",
      "loss: 0.396757  [38400/60000]\n",
      "loss: 0.528807  [44800/60000]\n",
      "loss: 0.530430  [51200/60000]\n",
      "loss: 0.371544  [57600/60000]\n",
      "Avg Loss: 0.410502\tAccuracy: 85.36%\n",
      "Epoch 17\n",
      "-------------\n",
      "loss: 0.311417  [    0/60000]\n",
      "loss: 0.332368  [ 6400/60000]\n",
      "loss: 0.226439  [12800/60000]\n",
      "loss: 0.436042  [19200/60000]\n",
      "loss: 0.397694  [25600/60000]\n",
      "loss: 0.414111  [32000/60000]\n",
      "loss: 0.388782  [38400/60000]\n",
      "loss: 0.524150  [44800/60000]\n",
      "loss: 0.511967  [51200/60000]\n",
      "loss: 0.367500  [57600/60000]\n",
      "Avg Loss: 0.404040\tAccuracy: 85.57%\n",
      "Epoch 18\n",
      "-------------\n",
      "loss: 0.300795  [    0/60000]\n",
      "loss: 0.320294  [ 6400/60000]\n",
      "loss: 0.230649  [12800/60000]\n",
      "loss: 0.420649  [19200/60000]\n",
      "loss: 0.387449  [25600/60000]\n",
      "loss: 0.403799  [32000/60000]\n",
      "loss: 0.382561  [38400/60000]\n",
      "loss: 0.531147  [44800/60000]\n",
      "loss: 0.510847  [51200/60000]\n",
      "loss: 0.363637  [57600/60000]\n",
      "Avg Loss: 0.398449\tAccuracy: 85.83%\n",
      "Epoch 19\n",
      "-------------\n",
      "loss: 0.289325  [    0/60000]\n",
      "loss: 0.306875  [ 6400/60000]\n",
      "loss: 0.233259  [12800/60000]\n",
      "loss: 0.412125  [19200/60000]\n",
      "loss: 0.374460  [25600/60000]\n",
      "loss: 0.392931  [32000/60000]\n",
      "loss: 0.368061  [38400/60000]\n",
      "loss: 0.529079  [44800/60000]\n",
      "loss: 0.509516  [51200/60000]\n",
      "loss: 0.357615  [57600/60000]\n",
      "Avg Loss: 0.393181\tAccuracy: 86.02%\n",
      "Epoch 20\n",
      "-------------\n",
      "loss: 0.281889  [    0/60000]\n",
      "loss: 0.297330  [ 6400/60000]\n",
      "loss: 0.225952  [12800/60000]\n",
      "loss: 0.398753  [19200/60000]\n",
      "loss: 0.360505  [25600/60000]\n",
      "loss: 0.384198  [32000/60000]\n",
      "loss: 0.358780  [38400/60000]\n",
      "loss: 0.525007  [44800/60000]\n",
      "loss: 0.494984  [51200/60000]\n",
      "loss: 0.357645  [57600/60000]\n",
      "Avg Loss: 0.386726\tAccuracy: 86.18%\n",
      "Epoch 21\n",
      "-------------\n",
      "loss: 0.269894  [    0/60000]\n",
      "loss: 0.293010  [ 6400/60000]\n",
      "loss: 0.223627  [12800/60000]\n",
      "loss: 0.387568  [19200/60000]\n",
      "loss: 0.356298  [25600/60000]\n",
      "loss: 0.387789  [32000/60000]\n",
      "loss: 0.359687  [38400/60000]\n",
      "loss: 0.527336  [44800/60000]\n",
      "loss: 0.491159  [51200/60000]\n",
      "loss: 0.355405  [57600/60000]\n",
      "Avg Loss: 0.378685\tAccuracy: 86.47%\n",
      "Epoch 22\n",
      "-------------\n",
      "loss: 0.261391  [    0/60000]\n",
      "loss: 0.283098  [ 6400/60000]\n",
      "loss: 0.221365  [12800/60000]\n",
      "loss: 0.377082  [19200/60000]\n",
      "loss: 0.343733  [25600/60000]\n",
      "loss: 0.373118  [32000/60000]\n",
      "loss: 0.356003  [38400/60000]\n",
      "loss: 0.524468  [44800/60000]\n",
      "loss: 0.477177  [51200/60000]\n",
      "loss: 0.355032  [57600/60000]\n",
      "Avg Loss: 0.373984\tAccuracy: 86.63%\n",
      "Epoch 23\n",
      "-------------\n",
      "loss: 0.254681  [    0/60000]\n",
      "loss: 0.279913  [ 6400/60000]\n",
      "loss: 0.216233  [12800/60000]\n",
      "loss: 0.368776  [19200/60000]\n",
      "loss: 0.342290  [25600/60000]\n",
      "loss: 0.365823  [32000/60000]\n",
      "loss: 0.346330  [38400/60000]\n",
      "loss: 0.515266  [44800/60000]\n",
      "loss: 0.473110  [51200/60000]\n",
      "loss: 0.352601  [57600/60000]\n",
      "Avg Loss: 0.373090\tAccuracy: 86.57%\n",
      "Epoch 24\n",
      "-------------\n",
      "loss: 0.256116  [    0/60000]\n",
      "loss: 0.268829  [ 6400/60000]\n",
      "loss: 0.210719  [12800/60000]\n",
      "loss: 0.357423  [19200/60000]\n",
      "loss: 0.338933  [25600/60000]\n",
      "loss: 0.358386  [32000/60000]\n",
      "loss: 0.348012  [38400/60000]\n",
      "loss: 0.520432  [44800/60000]\n",
      "loss: 0.462405  [51200/60000]\n",
      "loss: 0.354703  [57600/60000]\n",
      "Avg Loss: 0.370731\tAccuracy: 86.74%\n",
      "Epoch 25\n",
      "-------------\n",
      "loss: 0.250816  [    0/60000]\n",
      "loss: 0.260603  [ 6400/60000]\n",
      "loss: 0.205160  [12800/60000]\n",
      "loss: 0.345960  [19200/60000]\n",
      "loss: 0.333770  [25600/60000]\n",
      "loss: 0.355092  [32000/60000]\n",
      "loss: 0.344610  [38400/60000]\n",
      "loss: 0.520930  [44800/60000]\n",
      "loss: 0.458326  [51200/60000]\n",
      "loss: 0.353762  [57600/60000]\n",
      "Avg Loss: 0.365399\tAccuracy: 86.86%\n",
      "Epoch 26\n",
      "-------------\n",
      "loss: 0.245983  [    0/60000]\n",
      "loss: 0.254893  [ 6400/60000]\n",
      "loss: 0.204953  [12800/60000]\n",
      "loss: 0.338743  [19200/60000]\n",
      "loss: 0.332393  [25600/60000]\n",
      "loss: 0.351267  [32000/60000]\n",
      "loss: 0.339811  [38400/60000]\n",
      "loss: 0.521651  [44800/60000]\n",
      "loss: 0.450000  [51200/60000]\n",
      "loss: 0.353106  [57600/60000]\n",
      "Avg Loss: 0.361602\tAccuracy: 87.00%\n",
      "Epoch 27\n",
      "-------------\n",
      "loss: 0.247458  [    0/60000]\n",
      "loss: 0.250805  [ 6400/60000]\n",
      "loss: 0.194688  [12800/60000]\n",
      "loss: 0.330887  [19200/60000]\n",
      "loss: 0.334359  [25600/60000]\n",
      "loss: 0.345097  [32000/60000]\n",
      "loss: 0.340795  [38400/60000]\n",
      "loss: 0.513286  [44800/60000]\n",
      "loss: 0.442293  [51200/60000]\n",
      "loss: 0.352407  [57600/60000]\n",
      "Avg Loss: 0.359213\tAccuracy: 87.14%\n",
      "Epoch 28\n",
      "-------------\n",
      "loss: 0.241105  [    0/60000]\n",
      "loss: 0.244194  [ 6400/60000]\n",
      "loss: 0.197477  [12800/60000]\n",
      "loss: 0.324873  [19200/60000]\n",
      "loss: 0.327609  [25600/60000]\n",
      "loss: 0.342995  [32000/60000]\n",
      "loss: 0.334181  [38400/60000]\n",
      "loss: 0.518132  [44800/60000]\n",
      "loss: 0.437289  [51200/60000]\n",
      "loss: 0.348480  [57600/60000]\n",
      "Avg Loss: 0.354137\tAccuracy: 87.28%\n",
      "Epoch 29\n",
      "-------------\n",
      "loss: 0.236157  [    0/60000]\n",
      "loss: 0.240757  [ 6400/60000]\n",
      "loss: 0.195085  [12800/60000]\n",
      "loss: 0.314740  [19200/60000]\n",
      "loss: 0.332508  [25600/60000]\n",
      "loss: 0.342472  [32000/60000]\n",
      "loss: 0.333043  [38400/60000]\n",
      "loss: 0.515551  [44800/60000]\n",
      "loss: 0.440015  [51200/60000]\n",
      "loss: 0.345100  [57600/60000]\n",
      "Avg Loss: 0.351884\tAccuracy: 87.39%\n",
      "Epoch 30\n",
      "-------------\n",
      "loss: 0.234291  [    0/60000]\n",
      "loss: 0.239205  [ 6400/60000]\n",
      "loss: 0.192251  [12800/60000]\n",
      "loss: 0.306507  [19200/60000]\n",
      "loss: 0.318658  [25600/60000]\n",
      "loss: 0.341642  [32000/60000]\n",
      "loss: 0.329376  [38400/60000]\n",
      "loss: 0.509453  [44800/60000]\n",
      "loss: 0.427695  [51200/60000]\n",
      "loss: 0.339991  [57600/60000]\n",
      "Avg Loss: 0.347947\tAccuracy: 87.51%\n",
      "Epoch 31\n",
      "-------------\n",
      "loss: 0.233064  [    0/60000]\n",
      "loss: 0.233192  [ 6400/60000]\n",
      "loss: 0.191475  [12800/60000]\n",
      "loss: 0.298687  [19200/60000]\n",
      "loss: 0.317289  [25600/60000]\n",
      "loss: 0.338180  [32000/60000]\n",
      "loss: 0.326737  [38400/60000]\n",
      "loss: 0.504524  [44800/60000]\n",
      "loss: 0.423196  [51200/60000]\n",
      "loss: 0.340841  [57600/60000]\n",
      "Avg Loss: 0.341998\tAccuracy: 87.68%\n",
      "Epoch 32\n",
      "-------------\n",
      "loss: 0.226433  [    0/60000]\n",
      "loss: 0.230873  [ 6400/60000]\n",
      "loss: 0.189343  [12800/60000]\n",
      "loss: 0.294593  [19200/60000]\n",
      "loss: 0.304780  [25600/60000]\n",
      "loss: 0.341491  [32000/60000]\n",
      "loss: 0.323379  [38400/60000]\n",
      "loss: 0.497485  [44800/60000]\n",
      "loss: 0.423805  [51200/60000]\n",
      "loss: 0.341657  [57600/60000]\n",
      "Avg Loss: 0.342686\tAccuracy: 87.58%\n",
      "Epoch 33\n",
      "-------------\n",
      "loss: 0.224941  [    0/60000]\n",
      "loss: 0.226563  [ 6400/60000]\n",
      "loss: 0.187925  [12800/60000]\n",
      "loss: 0.292913  [19200/60000]\n",
      "loss: 0.307735  [25600/60000]\n",
      "loss: 0.336928  [32000/60000]\n",
      "loss: 0.320601  [38400/60000]\n",
      "loss: 0.490942  [44800/60000]\n",
      "loss: 0.413322  [51200/60000]\n",
      "loss: 0.338706  [57600/60000]\n",
      "Avg Loss: 0.339251\tAccuracy: 87.81%\n",
      "Epoch 34\n",
      "-------------\n",
      "loss: 0.221277  [    0/60000]\n",
      "loss: 0.224273  [ 6400/60000]\n",
      "loss: 0.184621  [12800/60000]\n",
      "loss: 0.284661  [19200/60000]\n",
      "loss: 0.307676  [25600/60000]\n",
      "loss: 0.337322  [32000/60000]\n",
      "loss: 0.320477  [38400/60000]\n",
      "loss: 0.491153  [44800/60000]\n",
      "loss: 0.421607  [51200/60000]\n",
      "loss: 0.332737  [57600/60000]\n",
      "Avg Loss: 0.335038\tAccuracy: 87.98%\n",
      "Epoch 35\n",
      "-------------\n",
      "loss: 0.221284  [    0/60000]\n",
      "loss: 0.220761  [ 6400/60000]\n",
      "loss: 0.182932  [12800/60000]\n",
      "loss: 0.283078  [19200/60000]\n",
      "loss: 0.306548  [25600/60000]\n",
      "loss: 0.337035  [32000/60000]\n",
      "loss: 0.320773  [38400/60000]\n",
      "loss: 0.477508  [44800/60000]\n",
      "loss: 0.419754  [51200/60000]\n",
      "loss: 0.328215  [57600/60000]\n",
      "Avg Loss: 0.335323\tAccuracy: 88.00%\n",
      "Epoch 36\n",
      "-------------\n",
      "loss: 0.220277  [    0/60000]\n",
      "loss: 0.219683  [ 6400/60000]\n",
      "loss: 0.181510  [12800/60000]\n",
      "loss: 0.277956  [19200/60000]\n",
      "loss: 0.299614  [25600/60000]\n",
      "loss: 0.330210  [32000/60000]\n",
      "loss: 0.320980  [38400/60000]\n",
      "loss: 0.473274  [44800/60000]\n",
      "loss: 0.428597  [51200/60000]\n",
      "loss: 0.323507  [57600/60000]\n",
      "Avg Loss: 0.331599\tAccuracy: 88.16%\n",
      "Epoch 37\n",
      "-------------\n",
      "loss: 0.219497  [    0/60000]\n",
      "loss: 0.211955  [ 6400/60000]\n",
      "loss: 0.174824  [12800/60000]\n",
      "loss: 0.271752  [19200/60000]\n",
      "loss: 0.301845  [25600/60000]\n",
      "loss: 0.327665  [32000/60000]\n",
      "loss: 0.316764  [38400/60000]\n",
      "loss: 0.467717  [44800/60000]\n",
      "loss: 0.421916  [51200/60000]\n",
      "loss: 0.318460  [57600/60000]\n",
      "Avg Loss: 0.330954\tAccuracy: 88.03%\n",
      "Epoch 38\n",
      "-------------\n",
      "loss: 0.222082  [    0/60000]\n",
      "loss: 0.208190  [ 6400/60000]\n",
      "loss: 0.173527  [12800/60000]\n",
      "loss: 0.263577  [19200/60000]\n",
      "loss: 0.293620  [25600/60000]\n",
      "loss: 0.324639  [32000/60000]\n",
      "loss: 0.320097  [38400/60000]\n",
      "loss: 0.463644  [44800/60000]\n",
      "loss: 0.419751  [51200/60000]\n",
      "loss: 0.323090  [57600/60000]\n",
      "Avg Loss: 0.329459\tAccuracy: 88.08%\n",
      "Epoch 39\n",
      "-------------\n",
      "loss: 0.214056  [    0/60000]\n",
      "loss: 0.206338  [ 6400/60000]\n",
      "loss: 0.169860  [12800/60000]\n",
      "loss: 0.265158  [19200/60000]\n",
      "loss: 0.297191  [25600/60000]\n",
      "loss: 0.325261  [32000/60000]\n",
      "loss: 0.320265  [38400/60000]\n",
      "loss: 0.462851  [44800/60000]\n",
      "loss: 0.417804  [51200/60000]\n",
      "loss: 0.320311  [57600/60000]\n",
      "Avg Loss: 0.327566\tAccuracy: 88.22%\n",
      "Epoch 40\n",
      "-------------\n",
      "loss: 0.206465  [    0/60000]\n",
      "loss: 0.205009  [ 6400/60000]\n",
      "loss: 0.165637  [12800/60000]\n",
      "loss: 0.257523  [19200/60000]\n",
      "loss: 0.284840  [25600/60000]\n",
      "loss: 0.327278  [32000/60000]\n",
      "loss: 0.315853  [38400/60000]\n",
      "loss: 0.455022  [44800/60000]\n",
      "loss: 0.420023  [51200/60000]\n",
      "loss: 0.309002  [57600/60000]\n",
      "Avg Loss: 0.328687\tAccuracy: 88.21%\n",
      "Epoch 41\n",
      "-------------\n",
      "loss: 0.214022  [    0/60000]\n",
      "loss: 0.196671  [ 6400/60000]\n",
      "loss: 0.164762  [12800/60000]\n",
      "loss: 0.249509  [19200/60000]\n",
      "loss: 0.285389  [25600/60000]\n",
      "loss: 0.316844  [32000/60000]\n",
      "loss: 0.312414  [38400/60000]\n",
      "loss: 0.444334  [44800/60000]\n",
      "loss: 0.416685  [51200/60000]\n",
      "loss: 0.304856  [57600/60000]\n",
      "Avg Loss: 0.328811\tAccuracy: 88.22%\n",
      "Epoch 42\n",
      "-------------\n",
      "loss: 0.214499  [    0/60000]\n",
      "loss: 0.197557  [ 6400/60000]\n",
      "loss: 0.171527  [12800/60000]\n",
      "loss: 0.242494  [19200/60000]\n",
      "loss: 0.279141  [25600/60000]\n",
      "loss: 0.325139  [32000/60000]\n",
      "loss: 0.311685  [38400/60000]\n",
      "loss: 0.431541  [44800/60000]\n",
      "loss: 0.415899  [51200/60000]\n",
      "loss: 0.299656  [57600/60000]\n",
      "Avg Loss: 0.325874\tAccuracy: 88.27%\n",
      "Epoch 43\n",
      "-------------\n",
      "loss: 0.217737  [    0/60000]\n",
      "loss: 0.193936  [ 6400/60000]\n",
      "loss: 0.162405  [12800/60000]\n",
      "loss: 0.241614  [19200/60000]\n",
      "loss: 0.272505  [25600/60000]\n",
      "loss: 0.318133  [32000/60000]\n",
      "loss: 0.308766  [38400/60000]\n",
      "loss: 0.425846  [44800/60000]\n",
      "loss: 0.414270  [51200/60000]\n",
      "loss: 0.292600  [57600/60000]\n",
      "Avg Loss: 0.326148\tAccuracy: 88.28%\n",
      "Epoch 44\n",
      "-------------\n",
      "loss: 0.210168  [    0/60000]\n",
      "loss: 0.189676  [ 6400/60000]\n",
      "loss: 0.160203  [12800/60000]\n",
      "loss: 0.236206  [19200/60000]\n",
      "loss: 0.266268  [25600/60000]\n",
      "loss: 0.330159  [32000/60000]\n",
      "loss: 0.306132  [38400/60000]\n",
      "loss: 0.429221  [44800/60000]\n",
      "loss: 0.407195  [51200/60000]\n",
      "loss: 0.291322  [57600/60000]\n",
      "Avg Loss: 0.325629\tAccuracy: 88.33%\n",
      "Epoch 45\n",
      "-------------\n",
      "loss: 0.209555  [    0/60000]\n",
      "loss: 0.188521  [ 6400/60000]\n",
      "loss: 0.162326  [12800/60000]\n",
      "loss: 0.233565  [19200/60000]\n",
      "loss: 0.268981  [25600/60000]\n",
      "loss: 0.312323  [32000/60000]\n",
      "loss: 0.308894  [38400/60000]\n",
      "loss: 0.408475  [44800/60000]\n",
      "loss: 0.397051  [51200/60000]\n",
      "loss: 0.287730  [57600/60000]\n",
      "Avg Loss: 0.323725\tAccuracy: 88.41%\n",
      "Epoch 46\n",
      "-------------\n",
      "loss: 0.207773  [    0/60000]\n",
      "loss: 0.186332  [ 6400/60000]\n",
      "loss: 0.160260  [12800/60000]\n",
      "loss: 0.229642  [19200/60000]\n",
      "loss: 0.265006  [25600/60000]\n",
      "loss: 0.315021  [32000/60000]\n",
      "loss: 0.313184  [38400/60000]\n",
      "loss: 0.408345  [44800/60000]\n",
      "loss: 0.396225  [51200/60000]\n",
      "loss: 0.274100  [57600/60000]\n",
      "Avg Loss: 0.321968\tAccuracy: 88.47%\n",
      "Epoch 47\n",
      "-------------\n",
      "loss: 0.205819  [    0/60000]\n",
      "loss: 0.183176  [ 6400/60000]\n",
      "loss: 0.158003  [12800/60000]\n",
      "loss: 0.221555  [19200/60000]\n",
      "loss: 0.266420  [25600/60000]\n",
      "loss: 0.319979  [32000/60000]\n",
      "loss: 0.308602  [38400/60000]\n",
      "loss: 0.403010  [44800/60000]\n",
      "loss: 0.391616  [51200/60000]\n",
      "loss: 0.273591  [57600/60000]\n",
      "Avg Loss: 0.322196\tAccuracy: 88.43%\n",
      "Epoch 48\n",
      "-------------\n",
      "loss: 0.207296  [    0/60000]\n",
      "loss: 0.177754  [ 6400/60000]\n",
      "loss: 0.158253  [12800/60000]\n",
      "loss: 0.217335  [19200/60000]\n",
      "loss: 0.263160  [25600/60000]\n",
      "loss: 0.316287  [32000/60000]\n",
      "loss: 0.311519  [38400/60000]\n",
      "loss: 0.397861  [44800/60000]\n",
      "loss: 0.388921  [51200/60000]\n",
      "loss: 0.277033  [57600/60000]\n",
      "Avg Loss: 0.321084\tAccuracy: 88.51%\n",
      "Epoch 49\n",
      "-------------\n",
      "loss: 0.208216  [    0/60000]\n",
      "loss: 0.178385  [ 6400/60000]\n",
      "loss: 0.161421  [12800/60000]\n",
      "loss: 0.209411  [19200/60000]\n",
      "loss: 0.259453  [25600/60000]\n",
      "loss: 0.311480  [32000/60000]\n",
      "loss: 0.311598  [38400/60000]\n",
      "loss: 0.396635  [44800/60000]\n",
      "loss: 0.382210  [51200/60000]\n",
      "loss: 0.274017  [57600/60000]\n",
      "Avg Loss: 0.319575\tAccuracy: 88.55%\n",
      "Epoch 50\n",
      "-------------\n",
      "loss: 0.203856  [    0/60000]\n",
      "loss: 0.175015  [ 6400/60000]\n",
      "loss: 0.161999  [12800/60000]\n",
      "loss: 0.206870  [19200/60000]\n",
      "loss: 0.258642  [25600/60000]\n",
      "loss: 0.312040  [32000/60000]\n",
      "loss: 0.311076  [38400/60000]\n",
      "loss: 0.386183  [44800/60000]\n",
      "loss: 0.375799  [51200/60000]\n",
      "loss: 0.268662  [57600/60000]\n",
      "Avg Loss: 0.318430\tAccuracy: 88.65%\n"
     ]
    }
   ],
   "source": [
    "epoches = 50\n",
    "for epoch in range(epoches):\n",
    "    print(\"Epoch {}\\n-------------\".format(epoch+1))\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotLoss(epoches, loss_list):\n",
    "    plt.scatter(range(epoches), loss_list, c='r', s=1, label='Lost of epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Loss with training')\n",
    "    plt.show()\n",
    "\n",
    "plotLoss(epoches, loss_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d59d48a9db7df9bd35d694d7b177ff1ada75e91fd938675c066878f09fd91e11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
